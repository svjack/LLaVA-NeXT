<p align="center" width="100%">
<img src="https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png"  width="80%" height="80%">
</p>

# LLaVA-NeXT: Open Large Multimodal Models
[![Static Badge](https://img.shields.io/badge/llava_video-paper-green)](http://arxiv.org/abs/2410.02713)
[![Static Badge](https://img.shields.io/badge/llava_onevision-paper-green)](https://arxiv.org/abs/2408.03326)
[![llava_next-blog](https://img.shields.io/badge/llava_next-blog-green)](https://llava-vl.github.io/blog/)

[![llava_onevision-demo](https://img.shields.io/badge/llava_onevision-demo-red)](https://llava-onevision.lmms-lab.com/)
[![llava_next-video_demo](https://img.shields.io/badge/llava_video-demo-red)](https://huggingface.co/spaces/WildVision/vision-arena)
[![llava_next-interleave_demo](https://img.shields.io/badge/llava_next-interleave_demo-red)](https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo)
[![Openbayes Demo](https://img.shields.io/static/v1?label=Demo&message=OpenBayes%E8%B4%9D%E5%BC%8F%E8%AE%A1%E7%AE%97&color=green)](https://openbayes.com/console/public/tutorials/gW0ng9jKXfO)

[![llava_video-checkpoints](https://img.shields.io/badge/llava_video-checkpoints-blue)](https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944)
[![llava_onevision-checkpoints](https://img.shields.io/badge/llava_onevision-checkpoints-blue)](https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37)
[![llava_next-interleave_checkpoints](https://img.shields.io/badge/llava_next-interleave_checkpoints-blue)](https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1)
[![llava_next-image_checkpoints](https://img.shields.io/badge/llava_next-image_checkpoints-blue)](https://huggingface.co/lmms-lab)

# LLaVA-Video-7B-Qwen2 and LLaVA-NeXT

This repository provides instructions and scripts to set up and run the LLaVA-Video-7B-Qwen2 model using the LLaVA-NeXT framework. The process involves cloning the repository, setting up the environment, and running a video captioning script. Additionally, it includes steps to generate metadata and upload the dataset to Hugging Face.

## Table of Contents
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Running the Video Captioning Script](#running-the-video-captioning-script)
- [Generating Metadata](#generating-metadata)
- [Uploading to Hugging Face](#uploading-to-hugging-face)

## Prerequisites

Before you begin, ensure you have the following installed:
- `git-lfs`
- `ffmpeg`
- `cbm`

You can install these dependencies using the following command:

```bash
sudo apt-get update && sudo apt-get install git-lfs ffmpeg cbm
sudo apt-get upgrade ffmpeg
```

## Installation

1. Clone the LLaVA-NeXT repository:

```bash
git clone https://github.com/svjack/LLaVA-NeXT
cd LLaVA-NeXT
```

2. Set up the conda environment:

```bash
conda create -n llava python=3.10 -y
conda activate llava
```

3. Install the necessary Python packages:

```bash
pip install ipykernel
python -m ipykernel install --user --name llava --display-name "llava"
pip install --upgrade pip  # Enable PEP 660 support.
pip install torch
pip install -e ".[train]"
pip install flash_attn==2.3.4 --no-build-isolation
pip install moviepy=="1.0.3"
```

## Running the Video Captioning Script

### To run the video captioning script, use the following command: 
### videos in dir have length 6s and [640, 360] as resolution 

- length 6s
### (refer to https://github.com/svjack/WatermarkRemover: python .\video_skipper.py  --input  .\åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰© -s 5 -e 10 -m 6)
- Resize [640, 360]
```python
import os
from moviepy.editor import VideoFileClip

def resize_videos_in_folder(input_folder, output_folder, new_size=(640, 360)):
    """
    å°†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶è°ƒæ•´å¤§å°å¹¶ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶å¤¹ä¸­ã€‚

    :param input_folder: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼ŒåŒ…å«è¦è°ƒæ•´å¤§å°çš„è§†é¢‘æ–‡ä»¶ã€‚
    :param output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œç”¨äºä¿å­˜è°ƒæ•´å¤§å°åçš„è§†é¢‘æ–‡ä»¶ã€‚
    :param new_size: æ–°çš„è§†é¢‘å°ºå¯¸ï¼Œæ ¼å¼ä¸º (width, height)ã€‚
    """
    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # éå†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(input_folder):
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶ï¼ˆè¿™é‡Œå‡è®¾è§†é¢‘æ–‡ä»¶ä¸ºå¸¸è§çš„è§†é¢‘æ ¼å¼ï¼‰
        if filename.endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')):
            input_path = os.path.join(input_folder, filename)
            output_path = os.path.join(output_folder, filename)

            # ä½¿ç”¨ moviepy åŠ è½½è§†é¢‘æ–‡ä»¶
            video_clip = VideoFileClip(input_path)

            # è°ƒæ•´è§†é¢‘å¤§å°
            resized_clip = video_clip.resize(new_size)

            # ä¿å­˜è°ƒæ•´å¤§å°åçš„è§†é¢‘
            resized_clip.write_videofile(output_path, codec='libx264')

            # å…³é—­è§†é¢‘å‰ªè¾‘ä»¥é‡Šæ”¾èµ„æº
            video_clip.close()
            resized_clip.close()

# ç¤ºä¾‹ç”¨æ³•
input_folder = 'C:/Users/DELL/Downloads/WatermarkRemover-master/WatermarkRemover-master/åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_skip/'
output_folder = 'C:/Users/DELL/Downloads/WatermarkRemover-master/WatermarkRemover-master/åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_processed'
new_size = (640, 360)  # æ–°çš„è§†é¢‘å°ºå¯¸

resize_videos_in_folder(input_folder, output_folder, new_size)
```


```bash
python llava_qwen_video_caption.py --input_path "åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_resized" --output_path "åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_captioned" --max_frames 19 --fps 1 --force_sample
```

## Generating Metadata

After running the captioning script, you can generate metadata for the processed videos. The following script will create a `metadata.csv` file:

```bash
#!/bin/bash

# æºç›®å½•å’Œç›®æ ‡ç›®å½•
src_dir="Toradora_Videos_Omni_Captioned"
dst_dir0="Toradora_Videos_Omni_Captioned_0"
dst_dir1="Toradora_Videos_Omni_Captioned_1"

# åˆ›å»ºç›®æ ‡ç›®å½•
mkdir -p "$dst_dir0" "$dst_dir1"

# ä½¿ç”¨å¸¦å¼•å·çš„å˜é‡å’ŒIFSå¤„ç†æ–‡ä»¶åä¸­çš„ç©ºæ ¼
IFS=$'\n'

# è·å–æ‰€æœ‰.mp4æ–‡ä»¶å¹¶æŒ‰æ–‡ä»¶åæ’åº
files=($(find "$src_dir" -maxdepth 1 -name "*.mp4" -print0 | sort -z | xargs -0 printf "%s\n"))

# è®¡ç®—æ–‡ä»¶æ€»æ•°å’Œä¸­é—´ç‚¹
total_files=${#files[@]}
half_point=$((total_files / 2))

# å¤åˆ¶å‰ä¸€åŠåˆ°dst_dir0
for ((i=0; i<half_point; i++)); do
    file="${files[i]}"
    base_name=$(basename "$file" .mp4)
    cp "$file" "$dst_dir0/"
    txt_file="$src_dir/$base_name.txt"
    if [ -f "$txt_file" ]; then
        cp "$txt_file" "$dst_dir0/"
    fi
done

# å¤åˆ¶åä¸€åŠåˆ°dst_dir1
for ((i=half_point; i<total_files; i++)); do
    file="${files[i]}"
    base_name=$(basename "$file" .mp4)
    cp "$file" "$dst_dir1/"
    txt_file="$src_dir/$base_name.txt"
    if [ -f "$txt_file" ]; then
        cp "$txt_file" "$dst_dir1/"
    fi
done

echo "æ–‡ä»¶å·²æˆåŠŸåˆ†å‰²å¹¶å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•"
```

```python
import pathlib
import pandas as pd

def r_func(txt_path):
    with open(txt_path, "r", encoding="utf-8") as f:
        return f.read().strip()

def generate_metadata(input_dir):
    # åˆ›å»ºPathå¯¹è±¡å¹¶æ ‡å‡†åŒ–è·¯å¾„
    input_path = pathlib.Path(input_dir).resolve()
    
    # æ”¶é›†æ‰€æœ‰è§†é¢‘å’Œæ–‡æœ¬æ–‡ä»¶
    file_list = []
    for file_path in input_path.rglob("*"):
        if file_path.suffix.lower() in ('.mp4', '.txt'):
            file_list.append({
                "stem": file_path.stem,
                "path": file_path,
                "type": "video" if file_path.suffix.lower() == '.mp4' else "text"
            })
    
    # åˆ›å»ºDataFrameå¹¶åˆ†ç»„å¤„ç†
    df = pd.DataFrame(file_list)
    grouped = df.groupby('stem')
    
    metadata = []
    for stem, group in grouped:
        # è·å–ç»„å†…æ–‡ä»¶
        videos = group[group['type'] == 'video']
        texts = group[group['type'] == 'text']
        
        # ç¡®ä¿æ¯ç»„æœ‰ä¸”åªæœ‰ä¸€ä¸ªè§†é¢‘å’Œä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶
        if len(videos) == 1 and len(texts) == 1:
            video_path = videos.iloc[0]['path']
            text_path = texts.iloc[0]['path']
            
            metadata.append({
                "file_name": video_path.name,  # è‡ªåŠ¨å¤„ç†ä¸åŒç³»ç»Ÿçš„æ–‡ä»¶å
                "prompt": r_func(text_path)
            })
    
    # ä¿å­˜ç»“æœåˆ°CSV
    output_path = input_path.parent / "metadata.csv"
    pd.DataFrame(metadata).to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"Metadata generated at: {output_path}")

'''
---
configs:
- config_name: default
  data_files:
  - split: train
    path: 
    - "*.mp4"
    - "metadata.csv"
---
'''
```

```python
pip install -U datasets

def r_func(x):
    with open(x, "r") as f:
        return f.read().strip()

import pathlib
import pandas as pd
import numpy as np

pd.DataFrame(
    pd.DataFrame(
        pd.Series(
            list(pathlib.Path("C:/Users/DELL/Downloads/åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_captioned/").rglob("*"))
        ).map(
            str
        ).map(lambda x: x if x.endswith("mp4") or x.endswith("txt") else np.nan).dropna().map(
            lambda x: (x.split("\\")[-1].split(".")[0], x)
        ).values.tolist()
    ).groupby(0)[1].apply(list).map(
        lambda l: list(map(lambda x: x.split("\\")[-1] if x.endswith(".mp4") else x, l))
    ).map(
        lambda l: {
            "file_name": list(filter(lambda x: x.endswith(".mp4"), l))[0],
            "prompt": r_func(list(filter(lambda x: x.endswith(".txt"), l))[0]),
        }
    ).values.tolist()
).to_csv("metadata.csv", index=False)

!cp metadata.csv åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_captioned
```

- OR Use function like

```python
import os
import shutil
import uuid
import pandas as pd
from pathlib import Path

def r_func(x):
    with open(x, "r", encoding="utf-8") as f:
        return f.read().strip()

def process_files(input_path, output_path, prefix=""):
    # åˆ›å»ºè¾“å‡ºè·¯å¾„
    os.makedirs(output_path, exist_ok=True)

    # è·å–æ‰€æœ‰mp4å’Œtxtæ–‡ä»¶
    files = list(Path(input_path).rglob("*"))
    mp4_files = [str(f) for f in files if f.suffix == ".mp4"]
    txt_files = [str(f) for f in files if f.suffix == ".txt"]

    # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æˆå¯¹å„¿çš„æ–‡ä»¶
    file_pairs = {}

    # éå†mp4æ–‡ä»¶ï¼Œæ‰¾åˆ°å¯¹åº”çš„txtæ–‡ä»¶
    for mp4_file in mp4_files:
        base_name = Path(mp4_file).stem
        txt_file = next((f for f in txt_files if Path(f).stem == base_name), None)
        if txt_file:
            file_pairs[base_name] = (mp4_file, txt_file)

    # åˆ›å»ºmetadataåˆ—è¡¨
    metadata = []

    # å¤„ç†æ¯ä¸€å¯¹æ–‡ä»¶
    for base_name, (mp4_file, txt_file) in file_pairs.items():
        # ç”ŸæˆUUID
        unique_id = str(uuid.uuid4())

        # æ„å»ºæ–°çš„æ–‡ä»¶å
        new_mp4_file = Path(output_path) / f"{unique_id}.mp4"
        new_txt_file = Path(output_path) / f"{unique_id}.txt"

        # æ‹·è´mp4æ–‡ä»¶åˆ°æ–°è·¯å¾„å¹¶é‡å‘½å
        shutil.copy(mp4_file, new_mp4_file)

        # è¯»å–txtæ–‡ä»¶å†…å®¹
        prompt = r_func(txt_file)

        # åœ¨å†…å®¹å‰æ·»åŠ prefix
        modified_prompt = f"{prefix}{prompt}"

        # å°†ä¿®æ”¹åçš„å†…å®¹å†™å…¥æ–°çš„txtæ–‡ä»¶
        with open(new_txt_file, "w", encoding="utf-8") as f:
            f.write(modified_prompt)

        # æ·»åŠ åˆ°metadataåˆ—è¡¨
        metadata.append({
            "file_name": f"{unique_id}.mp4",
            "prompt": modified_prompt,
            "original_file_name": base_name,  # æ·»åŠ é‡å‘½åå‰çš„æ–‡ä»¶å
        })

    # ç”Ÿæˆmetadata.csvæ–‡ä»¶
    df = pd.DataFrame(metadata)
    df.to_csv(Path(output_path) / "metadata.csv", index=False)

# ç¤ºä¾‹è°ƒç”¨
input_path = "é£ç‰©é›†_captioned"
output_path = "é£ç‰©é›†_processed"
process_files(input_path, output_path)
```
- OR
```python
import os
import shutil
import uuid
import pandas as pd
from pathlib import Path
from tqdm import tqdm  # å¯¼å…¥ tqdm ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

def r_func(x):
    with open(x, "r", encoding="utf-8") as f:
        return f.read().strip()

def process_files(input_path, output_path, evaluation_results_path, prefix=""):
    # åˆ›å»ºè¾“å‡ºè·¯å¾„
    os.makedirs(output_path, exist_ok=True)

    # è¯»å– evaluation_results.csv æ–‡ä»¶
    evaluation_df = pd.read_csv(evaluation_results_path)

    # åˆ›å»ºmetadataåˆ—è¡¨
    metadata = []

    # ä½¿ç”¨ tqdm åŒ…è£…å¾ªç¯ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
    for index, row in tqdm(evaluation_df.iterrows(), total=len(evaluation_df), desc="Processing files"):
        video_name = row['video_name']
        mp4_file = Path(input_path) / f"{video_name}"
        txt_file = Path(input_path) / f"{Path(video_name).stem}.txt"

        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å’Œæ–‡æœ¬æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if mp4_file.exists() and txt_file.exists():
            # ç”ŸæˆUUID
            unique_id = str(uuid.uuid4())

            # æ„å»ºæ–°çš„æ–‡ä»¶å
            new_mp4_file = Path(output_path) / f"{unique_id}.mp4"
            new_txt_file = Path(output_path) / f"{unique_id}.txt"

            # æ‹·è´mp4æ–‡ä»¶åˆ°æ–°è·¯å¾„å¹¶é‡å‘½å
            shutil.copy(mp4_file, new_mp4_file)

            # è¯»å–txtæ–‡ä»¶å†…å®¹
            prompt = r_func(txt_file)

            # åœ¨å†…å®¹å‰æ·»åŠ prefix
            modified_prompt = f"{prefix}{prompt}"

            # å°†ä¿®æ”¹åçš„å†…å®¹å†™å…¥æ–°çš„txtæ–‡ä»¶
            with open(new_txt_file, "w", encoding="utf-8") as f:
                f.write(modified_prompt)

            # æ·»åŠ åˆ°metadataåˆ—è¡¨
            metadata_entry = {
                "file_name": f"{unique_id}.mp4",
                "prompt": modified_prompt,
                "original_file_name": video_name,  # æ·»åŠ é‡å‘½åå‰çš„æ–‡ä»¶å
            }

            # æ·»åŠ  evaluation_results.csv ä¸­å¯¹åº”è¡Œçš„æ‰€æœ‰é”®å€¼ï¼ˆé™¤äº† video_nameï¼‰
            for key, value in row.items():
                if key != 'video_name':
                    metadata_entry[key] = value

            metadata.append(metadata_entry)

    # ç”Ÿæˆmetadata.csvæ–‡ä»¶
    df = pd.DataFrame(metadata)
    df.to_csv(Path(output_path) / "metadata.csv", index=False)

# ç¤ºä¾‹è°ƒç”¨
input_path = "../videos_captioned/"
output_path = "../videos_upload/"
evaluation_results_path = "evaluation_results.csv"  # å¯è¾“å…¥å‚æ•°
process_files(input_path, output_path, evaluation_results_path)

from moviepy.editor import VideoFileClip
import numpy as np

def detect_black_scenes(video_path, black_threshold=0.1, frame_threshold=0.9):
    """
    æ£€æµ‹è§†é¢‘ä¸­æ˜¯å¦å­˜åœ¨é»‘è‰²åœºæ™¯ã€‚

    :param video_path: è§†é¢‘æ–‡ä»¶çš„è·¯å¾„
    :param black_threshold: åˆ¤æ–­ä¸ºé»‘è‰²çš„äº®åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œé»˜è®¤0.1
    :param frame_threshold: åˆ¤æ–­ä¸ºé»‘è‰²åœºæ™¯çš„å¸§æ¯”ä¾‹é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œé»˜è®¤0.9
    :return: å¦‚æœè§†é¢‘ä¸­å­˜åœ¨é»‘è‰²åœºæ™¯ï¼Œè¿”å› Trueï¼›å¦åˆ™è¿”å› False
    """
    # åŠ è½½è§†é¢‘
    clip = VideoFileClip(video_path)
    
    # åˆå§‹åŒ–é»‘è‰²åœºæ™¯è®¡æ•°å™¨
    black_frames = 0
    total_frames = 0
    
    # éå†è§†é¢‘çš„æ¯ä¸€å¸§
    for frame in clip.iter_frames():
        total_frames += 1
        # è®¡ç®—å¸§çš„å¹³å‡äº®åº¦
        brightness = np.mean(frame) / 255  # å°†äº®åº¦å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´
        if brightness < black_threshold:
            black_frames += 1
    
    # è®¡ç®—é»‘è‰²å¸§çš„æ¯”ä¾‹
    black_frame_ratio = black_frames / total_frames
    return black_frame_ratio
    # åˆ¤æ–­æ˜¯å¦å­˜åœ¨é»‘è‰²åœºæ™¯
    if black_frame_ratio >= frame_threshold:
        return True
    else:
        return False

df = pd.read_csv("../videos_upload/metadata.csv")
from tqdm import tqdm 
import os
req = []
for i, r in tqdm(df.iterrows()):
    d = r.to_dict()
    d["black_ratio"] = detect_black_scenes(os.path.join("../videos_upload", d["file_name"]))
    req.append(d)

df_with_score = pd.DataFrame(req)
df_with_score.to_csv("../metadata.csv", index = False)

!cp ../metadata.csv videos_upload
!huggingface_cli login
!huggingface-cli upload svjack/Genshin-Impact-Cutscenes-with-score-organized videos_upload . --repo-type dataset
```

## Uploading to Hugging Face

1. Load the dataset using the `datasets` library:

```python
from datasets import load_dataset
video_ds = load_dataset("videofolder", data_dir="åŸç¥é£æ™¯è§†é¢‘ï¼ˆå»æ°´å°ï¼‰1920x1080_äººç‰©_captioned/")
video_ds
```

2. Log in to Hugging Face CLI:

```bash
!huggingface-cli login
```

3. Upload the dataset to Hugging Face:

```bash
svjack/video-dataset-genshin-impact-landscape-organized
```

4. Upload the `metadata.csv` file and create a `readme.md` file similar to the example provided in [sayakpaul/video-dataset-disney-organized](https://huggingface.co/datasets/sayakpaul/video-dataset-disney-organized).

## Conclusion

This guide provides a step-by-step process to set up and run the LLaVA-Video-7B-Qwen2 model using the LLaVA-NeXT framework. It also includes instructions on how to generate metadata and upload the dataset to Hugging Face.

## Release Notes

- **[2024/10/04] ğŸ”¥ LLaVA-Video** (formerly LLaVA-NeXT-Video) has undergone a major upgrade! We are excited to release **LLaVA-Video-178K**, a high-quality synthetic dataset for video instruction tuning. This dataset includes:

  - 178,510 caption entries
  - 960,792 open-ended Q&A pairs
  - 196,198 multiple-choice Q&A items

  Along with this, weâ€™re also releasing the **LLaVA-Video 7B/72B models**, which deliver competitive performance on the latest video benchmarks, including [Video-MME](https://video-mme.github.io/home_page.html#leaderboard), [LongVideoBench](https://longvideobench.github.io/), and [Dream-1K](https://tarsier-vlm.github.io/).

  ğŸ“„ **Explore more**:
  - [LLaVA-Video-178K Dataset](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K): Download the dataset.
  - [LLaVA-Video Models](https://huggingface.co/collections/lmms-lab/llava-video-661e86f5e8dabc3ff793c944): Access model checkpoints.
  - [Paper](http://arxiv.org/abs/2410.02713): Detailed information about LLaVA-Video.
  - [LLaVA-Video Documentation](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_Video_1003.md): Guidance on training, inference and evaluation.

- [2024/09/13] ğŸ”¥ **ğŸš€ [LLaVA-OneVision-Chat](docs/LLaVA_OneVision_Chat.md)**. The new LLaVA-OV-Chat (7B/72B) significantly improves the chat experience of LLaVA-OV. ğŸ“„
  
  ![](docs/ov_chat_images/chat_results.png)

- [2024/08/06] ğŸ”¥ **ğŸš€ [LLaVA-OneVision (OV)](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/)!** The new LLaVA-OV models (0.5B/7B/72B) achieve new state-of-the-art performance across single-image, multi-image, and video benchmarks, sometimes rivaling top commercial models on 47 diverse benchmarks. ğŸ“„ Explore More:
  * [[Paper]](https://arxiv.org/abs/2408.03326): In-depth insights, new emegerging scenarios, ie, strong video understadning through task transfer from images.
  * [[LLaVA-OV Doc]](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision.md): Model inference and evaluation guidance.
  * [[Scripts]](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/scripts/train): Start training models on your single-image/multi-image/video data.
    
- [2024/07/16] ğŸ”¥ **LLaVA-NeXT-Video** has been upgraded. The new 32B model achieves the best open-source performance on several video benchmarks, including [Video-MME](https://video-mme.github.io/home_page.html#leaderboard). Please refer to [this page](docs/LLaVA-NeXT-Video_0716.md) for details, refer to [llava_next-video_demo](https://huggingface.co/spaces/WildVision/vision-arena) for demo.


- [2024/06/23] ğŸ”¥ **LLaVA-NeXT-Interleave** is released. We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM and achieve **SoTA** performance on a wide range of benchmarks. Check out [paper](https://arxiv.org/pdf/2407.07895), [blog](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/), and [checkpoints](https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1) to see new capabilities and improved performance! We have released 0.5b, 7b, and 7b-dpo models.
  * An all-round LLM for multi-image, video, and 3D with strong performance \[[demo](https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo)\]
  * Construct interleave training data [**M4-Instruct**](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data)
  * Construct multi-image benchmark [**LLaVA-Interleave Bench**](https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench)


- [2024/05/25] ğŸ”¥ Wondering "[What Else Influences Visual Instruction Tuning Beyond Data?](https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/)" Our new [blog](https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/) summarizes empirical explorations to ablate the various design choices in improving LMMs except instruct data itself. Meanwhile, open-source the recapioned high-quality data using LLaVA-NeXT-34B on [[COCO]](https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-118K) [[LCS]](https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K) [[CC3M]](https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-CC3M).
  * Architectures (LMM & Vision Encoder)
  * Visual Representations (Resolution & # Tokens)
  * Training Strategies (High-quality data & Trainable modules)

- [2024/05/10] ğŸ”¥ **LLaVA-NeXT** (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [[blog](https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/)] and [[checkpoints](https://huggingface.co/lmms-lab)] to see improved performance!
- [2024/05/10] ğŸ”¥ **LLaVA-NeXT** (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [[Blog](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/)], [[checkpoints](https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944)] and [[sglang](https://github.com/sgl-project/sglang)]
- [2024/01/30] ğŸ”¥ **LLaVA-NeXT** is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the [blog post](https://llava-vl.github.io/blog/2024-01-30-llava-next/), and explore the [demo](https://llava.hliu.cc/)! Models are available in [Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md). Training/eval data and scripts coming soon.
<details>
<summary>More</summary>
  
- [2024/03/10] ğŸ”¥ Releasing **LMMs-Eval**, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [[Blog](https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/)] [[Codebase](https://github.com/EvolvingLMMs-Lab/lmms-eval)]
  
- [2023/11/10] [LLaVA-Plus](https://llava-vl.github.io/llava-plus/) is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [[Project Page](https://llava-vl.github.io/llava-plus/)] [[Demo](https://llavaplus.ngrok.io/)] [[Code](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase)] [[Paper](https://arxiv.org/abs/2311.05437)]
- [2023/11/02] [LLaVA-Interactive](https://llava-vl.github.io/llava-interactive/) is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [[Project Page](https://llava-vl.github.io/llava-interactive/)] [[Demo](https://llavainteractive.ngrok.io/)] [[Code](https://github.com/LLaVA-VL/LLaVA-Interactive-Demo)] [[Paper](https://arxiv.org/abs/2311.00571)]
- [2023/10/26] ğŸ”¥ LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement ([ckpts](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15), [script](https://github.com/haotian-liu/LLaVA#train)). We also provide a [doc](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md) on how to finetune LLaVA-1.5 on your own dataset with LoRA.
- [2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [[ğŸ¤— Demo](https://huggingface.co/spaces/etri-vilab/Ko-LLaVA)]
- [2023/10/05] ğŸ”¥ LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the [technical report](https://arxiv.org/abs/2310.03744), and explore the [demo](https://llava.hliu.cc/)! Models are available in [Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md). The training data and scripts of LLaVA-1.5 are released [here](https://github.com/haotian-liu/LLaVA#train), and evaluation scripts are released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md)!
- [2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project [[LLavA-RLHF]](https://llava-rlhf.github.io/)
- [2023/09/22] [LLaVA](https://arxiv.org/abs/2304.08485) is accepted by NeurIPS 2023 as **oral presentation**, and [LLaVA-Med](https://arxiv.org/abs/2306.00890) is accepted by NeurIPS 2023 Datasets and Benchmarks Track as **spotlight presentation**.
- [2023/11/06] Support **Intel** dGPU and CPU platforms. [More details here.](https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel)
- [2023/10/12] LLaVA is now supported in [llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3436) with 4-bit / 5-bit quantization support!
- [2023/10/11] The training data and scripts of LLaVA-1.5 are released [here](https://github.com/haotian-liu/LLaVA#train), and evaluation scripts are released [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md)!
- [2023/10/10] [Roboflow Deep Dive](https://blog.roboflow.com/first-impressions-with-llava-1-5/): First Impressions with LLaVA-1.5.
- [2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a [note](https://arxiv.org/abs/2309.09958). Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper [``Multimodal Foundation Models: From Specialists to General-Purpose Assistants''.](https://arxiv.org/abs/2309.10020)
<p align="center">
  <img src="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/blob/main/images/mfm_evolution.jpeg?raw=true" width=50%/>
</p>

- [2023/07/19] ğŸ”¥ We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release [LLaVA Bench](https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md) for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out [LLaVA-from-LLaMA-2](https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_from_LLaMA2.md), and our [model zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)!
- [2023/06/26] [CVPR 2023 Tutorial](https://vlp-tutorial.github.io/) on **Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4**!  Please check out [[Slides](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf)] [[Notes](https://arxiv.org/abs/2306.14895)] [[YouTube](https://youtu.be/mkI7EPD1vp8)] [[Bilibli](https://www.bilibili.com/video/BV1Ng4y1T7v3/)].
- [2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support!  Please see documentations [here](./docs/LoRA.md).
- [2023/06/01] We released **LLaVA-Med: Large Language and Vision Assistant for Biomedicine**, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities.  Checkout the [paper](https://arxiv.org/abs/2306.00890) and [page](https://github.com/microsoft/LLaVA-Med).
- [2023/05/06] We are releasing [LLaVA-Lighting-MPT-7B-preview](https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview), based on MPT-7B-Chat!  See [here](#LLaVA-MPT-7b) for more details.
- [2023/05/02] ğŸ”¥ We are releasing LLaVA-Lighting!  Train a lite, multimodal GPT-4 with just $40 in 3 hours!  See [here](#train-llava-lightning) for more details.
- [2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM!  Try it out [here](https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava).
- [2023/04/17] ğŸ”¥ We released **LLaVA: Large Language and Vision Assistant**. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities.  Checkout the [paper](https://arxiv.org/abs/2304.08485) and [demo](https://llava.hliu.cc/).

</details>

<!-- <a href="https://llava.hliu.cc/"><img src="assets/demo.gif" width="70%"></a> -->

**Usage and License Notices**: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the [OpenAI Terms of Use](https://openai.com/policies/terms-of-use) for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. [Llama-1/2 community license](https://ai.meta.com/llama/license/) for LLaMA-2 and Vicuna-v1.5, [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE) and [Llama-3 Research License](https://llama.meta.com/llama3/license/)). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.

## Models & Scripts

### Installation

#### 1. **Clone this repository and navigate to the LLaVA folder:**
```bash
git clone https://github.com/LLaVA-VL/LLaVA-NeXT
cd LLaVA-NeXT
```

#### 2. **Install the inference package:**
```bash
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # Enable PEP 660 support.
pip install -e ".[train]"
```

### Project Navigation
Please checkout the following page for more inference & evaluation details.

#### - **LLaVA-OneVision: Easy Task Transfer**
- [LLaVA-OneVision]([./docs/LLaVA-NeXT.md](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision.md)): for demo inference. The evaluation code is in [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval).

#### - **LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild**
- [LLaVA-NeXT-Image](./docs/LLaVA-NeXT.md): for image demo inference and evaluation of stronger LMMs using [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval).


#### - LLaVA-NeXT: A Strong Zero-shot Video Understanding Model
- [LLaVA-NeXT-Video](./docs/LLaVA-NeXT-Video.md): for video inference and evaluation scripts. We recommend to use [LMMs-video](https://lmms-lab.github.io/posts/lmms-eval-0.2/) for evaluation.

#### - LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models
- [LLaVA-NeXT-Interleave](./docs/LLaVA-NeXT-Interleave.md): for multi-image demo and evaluation scripts.

## SGLang for SpeedUp Inference and Deployment

We use [SGLang](https://github.com/sgl-project/sglang) to speed up inference and deployment of LLaVA-NeXT. You could make LLaVA-NeXT as a backend API service with SGLang.

**Prepare Environment**:
    Following the instruction in the [sglang](https://github.com/sgl-project/sglang?tab=readme-ov-file#install)

### LLaVA-NeXT/OneVision

Checkout the HTTP Post/Get and SRT usage at [sglang/examples/runtime/llava_onevision](https://github.com/sgl-project/sglang/tree/main/examples/runtime/llava_onevision)

### LLaVA-NeXT (Video)

**Launch and Run on (K) Nodes**:
- Go to sglang project
    ```
    cd PATH_TO/sglang
    ```
- First node:
    ```sh
    bash examples/usage/llava_video/srt_example_llava_v.sh K 0 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
    (e.g. bash examples/usage/llava_video/srt_example_llava_v.sh K 0 examples/usage/llava_video/videos/Q98Z4OTh8RwmDonc.mp4 lmms-lab/LLaVA-NeXT-Video-7B-DPO 16)
    ```
- Second node:
    ```sh
    bash examples/usage/llava_video/srt_example_llava_v.sh K 1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
    ```
- The K node:
    ```sh
    bash examples/usage/llava_video/srt_example_llava_v.sh K K-1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO
    ```


## Citation

If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:
```bibtex
@article{li2024llava,
  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@misc{li2024llavanext-ablations,
	title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},
	url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},
	author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},
	month={May},
	year={2024}
}

@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}

@misc{zhang2024llavanext-video,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}
```

## Acknowledgement

- [Vicuna](https://github.com/lm-sys/FastChat): the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!
- The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): [Bo Li](https://brianboli.com/), [Dong Guo](https://www.linkedin.com/in/dongguoset/), [Feng Li](https://scholar.google.com/citations?hl=zh-CN&user=ybRe9GcAAAAJ&view_op=list_works&sortby=pubdate), [Hao Zhang](https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=en), [Kaichen Zhang](https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg), [Renrui Zhang](https://zrrskywalker.github.io/), [Yuanhan Zhang](https://zhangyuanhan-ai.github.io/), led by [Chunyuan Li](https://chunyuan.li/) and with the guidance and help from [Haotian Liu](https://hliu.cc/).
- The `ï»¿lmms-eval` framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.

## Related Projects

- [Instruction Tuning with GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
- [LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day](https://github.com/microsoft/LLaVA-Med)
- [Otter: In-Context Multi-Modal Instruction Tuning](https://github.com/Luodian/Otter)

For future project ideas, please check out:
- [SEEM: Segment Everything Everywhere All at Once](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
- [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) to detect, segment, and generate anything by marrying [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [Segment-Anything](https://github.com/facebookresearch/segment-anything).
